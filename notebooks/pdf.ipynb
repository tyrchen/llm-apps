{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81073758-ae88-404c-9655-60faabbfdb12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedc1975-1716-463c-8bd6-17427d83637a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8d70b4-a6ef-4aee-a201-c7cdf90df6db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = UnstructuredPDFLoader(\"gpt4-tech-report.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa00e86d-7810-40e9-88c0-e0e27521e9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
     ]
    }
   ],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5ae137-b625-429b-8734-38469b5254c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document(s) in your data\n",
      "There are 263628 characters in your document\n"
     ]
    }
   ],
   "source": [
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e737a3e-2bba-4320-942d-70faac1e025c",
   "metadata": {},
   "source": [
    "## Chun the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb19a982-ccc5-446e-804a-3435c90384a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc62f59-c567-442c-826b-251513ccbd88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 301 documents\n"
     ]
    }
   ],
   "source": [
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a106a5-c550-4809-92ce-72663025072a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tchen/arena/ai/notebooks/.venv/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c0010a-b12d-40e0-b767-1ec70bddc571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "PINECONE_API_KEY = os.environ['PINECONE_KEY']\n",
    "PINECONE_API_ENV = 'us-east4-gcp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99d185cd-487b-467e-b1be-d601f07ee295",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1efeca49-b6dd-4197-89e9-c0844be29692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_API_ENV  # next to api key in console\n",
    ")\n",
    "index_name = \"langchain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a3930cb-81bd-462e-9e58-13c4602b056b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a71a2c9b-0132-44df-9906-56dce22ee36b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Which areas does GPT-4 work best?\"\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27d65070-7395-443b-a0cf-24eb8757b398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I can help you with, feel free to ask.GPT-4', metadata={}),\n",
       " Document(page_content='Askell et al.2022Askell et al.2022gpt-3.5-basegpt-3.5-basegpt-3.5-turbogpt-4-basegpt-4-basegpt-40%10%20%30%40%50%60%70%ModelAccuracyAccuracy on adversarial questions (TruthfulQA mc1)Anthropic-LMgpt-3.5gpt-4Figure8:PerformanceofGPT-4onTruthfulQA.Accuracyisshownonthey-axis,higherisbetter.WecompareGPT-4underzero-shotprompting,few-shotprompting,andafterRLHFﬁne-tuning.GPT-4signiﬁcantlyoutperformsbothGPT-3.5andAskelletal[100].ﬁxestoplotlegendandtitle65', metadata={}),\n",
       " Document(page_content='wide scoring bins. For example although GPT-4 attains the highest possible score on AP Biology (5/5),\\n\\nthis is only shown in the plot as 85th percentile because 15 percent of test-takers achieve that score.\\n\\nGPT-4 exhibits human-level performance on the majority of these professional and academic exams.\\n\\nNotably, it passes a simulated version of the Uniform Bar Examination with a score in the top 10% of\\n\\ntest takers (Table 1, Figure 4).\\n\\nThe model’s capabilities on exams appear to stem primarily from the pre-training process and are not\\n\\nsigniﬁcantly affected by RLHF. On multiple choice questions, both the base GPT-4 model and the\\n\\nRLHF model perform equally well on average across the exams we tested (see Appendix B).\\n\\nWe also evaluated the pre-trained base GPT-4 model on traditional benchmarks designed for evaluating\\n\\nlanguage models. For each benchmark we report, we ran contamination checks for test data appearing', metadata={}),\n",
       " Document(page_content='GPT-4 signiﬁcantly reduces hallucinations relative to previous GPT-3.5 models (which have them-\\n\\nselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our\\n\\nlatest GPT-3.5 on our internal, adversarially-designed factuality evaluations (Figure 6).\\n\\nFigure 6. Performance of GPT-4 on nine internal adversarially-designed factuality evaluations. Accuracy\\n\\nis shown on the y-axis, higher is better. An accuracy of 1.0 means the model’s answers are judged to\\n\\nbe in agreement with human ideal responses for all questions in the eval. We compare GPT-4 to three\\n\\nearlier versions of ChatGPT [64] based on GPT-3.5; GPT-4 improves on the latest GPT-3.5 model by 19\\n\\npercentage points, with signiﬁcant gains across all topics.\\n\\nGPT-4 makes progress on public benchmarks like TruthfulQA [66], which tests the model’s ability to\\n\\nseparate fact from an adversarially-selected set of incorrect statements (Figure 7). These questions', metadata={})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6abf4-ac53-4ed0-9501-4880b181bd48",
   "metadata": {},
   "source": [
    "### Query docs to get answers back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b4fb55c-51c7-4232-9796-9c92bea0e4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e993b153-8731-4f67-80a7-0b1e4c9d2633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "785de241-dae2-45b3-9c62-876543d8818d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"GPT-4 在哪些领域超过 GPT-3?\"\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12895cf1-2149-4610-97c2-493b77e55e95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' GPT-4 outperforms GPT-3 in areas such as reasoning, knowledge retention, coding, simulated bar exams, traditional NLP benchmarks, and the MMLU benchmark.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "635515b9-7d10-4341-bca1-4b71fee48292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are the major topics for this paper?\"\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ccd5cc6-bc2d-49e8-a6ae-ff99f275f804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The major topics for this paper are the different types of exams and tests, such as the AP Art History (FRQ), AP World History (MCQ), Graduate Record Examination (GRE) Verbal, AP US Government (FRQ), AP Physics 2 (FRQ), AP US Government (MCQ), SAT EBRW – Reading Portion, MKSAP Questions (MCQ), AP Chemistry (MCQ), AP Statistics (FRQ), AP Psychology (MCQ), AP Chemistry (FRQ), AP Macroeconomics (MCQ), AP Statistics (MCQ), Certiﬁed Sommelier (theory knowledge), SAT Math (MCQ), AP Calculus BC (MCQ), AP Environmental Science (MCQ), Introductory Sommelier (theory knowledge), USNCO Local Section Exam 2022, Advanced Sommelier, (theory knowledge), AMC 12, AMC 10, AP Microeconomics (MCQ), USA Biolympiad Semiﬁnal Exam 2020, AP Biology (MCQ), AP Art History (MCQ), Uniform Bar Exam (MBE+MEE+MPT), SAT EBRW – Writing Portion, Leetcode (medium), Leetcode (hard), Le'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4cc9e-b665-4604-9a5c-68ae5a9f09b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
